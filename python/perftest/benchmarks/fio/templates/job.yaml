apiVersion: batch/v1
kind: Job
metadata:
  name: {{ name }}-fio
  labels:
    {{ subresource_labels(name) | indent(4) }}
spec:
  # Make sure that the correct number of clients can run in parallel
  # and expect them all to succeed to consider the job successful
  completions: {{ spec.clients }}
  parallelism: {{ spec.clients }}
  # Any failures should result in the failure of the job
  # This is important because a client that restarts won't be running
  #Â in parallel with the others and will distort the results
  # Combined with restartPolicy=Never in the pod spec, this should ensure that
  backoffLimit: 0
  template:
    metadata:
      labels:
        {{ subresource_labels(name) | indent(8) }}
    spec:
      {%- if spec.hostNetwork %}
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      {%- endif %}
      {%- if spec.podSecurityContext %}
      securityContext:
        {{ spec.podSecurityContext | toyaml | indent(8) }}
      {%- endif %}
      restartPolicy: Never
      containers:
        - name: fio
          image: {{ spec.image }}
          imagePullPolicy: {{ spec.imagePullPolicy }}
          env:
            - name: NUM_CLIENTS
              value: "{{ spec.clients }}"
            - name: JOB_NAME
              value: {{ name }}
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          volumeMounts:
            - name: jobs
              mountPath: /fio
              readOnly: true
            - name: scratch
              mountPath: /scratch
          {%- if spec.resources %}
          resources:
            {{ spec.resources | toyaml | indent(12) }}
          {%- endif %}
      volumes:
        - name: jobs
          configMap:
            name: {{ name }}-fio-config
        - name: scratch
          persistentVolumeClaim:
            claimName: {{ pvc_name }}
      # Set up spread constraints to try and spread the pods evenly across the nodes
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              {{ subresource_labels(name) | indent(14) }}
