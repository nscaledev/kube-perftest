apiVersion: batch/v1
kind: Job
metadata:
  name: {{ name }}-fio
  labels:
    {{ subresource_labels(name) | indent(4) }}
spec:
  completions: {{ spec.clients }}
  parallelism: {{ spec.clients }}
  template:
    metadata:
      labels:
        {{ subresource_labels(name) | indent(8) }}
    spec:
      {%- if spec.hostNetwork %}
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      {%- endif %}
      {%- if spec.podSecurityContext %}
      securityContext:
        {{ spec.podSecurityContext | toyaml | indent(8) }}
      {%- endif %}
      restartPolicy: Never
      containers:
        - name: fio
          image: {{ spec.image }}
          imagePullPolicy: {{ spec.imagePullPolicy }}
          env:
            - name: NUM_CLIENTS
              value: "{{ spec.clients }}"
            - name: JOB_NAME
              value: {{ name }}
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          volumeMounts:
            - name: jobs
              mountPath: /fio
              readOnly: true
            - name: scratch
              mountPath: /scratch
          # The container has successfully started once the data directory exists
          # livenessProbe:
          #   exec:
          #     command:
          #       - bash
          #       - -c
          #       - "test -f /scratch/${POD_NAME}/RUNNING"
          # readinessProbe:
          #   exec:
          #     command:
          #       - bash
          #       - -c
          #       - "test -f /scratch/${POD_NAME}/RUNNING"
          # startupProbe:
          #   exec:
          #     command:
          #       - bash
          #       - -c
          #       - "test -f /scratch/${POD_NAME}/RUNNING"
          #   # Allow up to 10m for the pods to synchronise
          #   periodSeconds: 1
          #   failureThreshold: 600
          {%- if spec.resources %}
          resources:
            {{ spec.resources | toyaml | indent(12) }}
          {%- endif %}
      volumes:
        - name: jobs
          configMap:
            name: {{ name }}-fio-config
        - name: scratch
          persistentVolumeClaim:
            claimName: {{ pvc_name }}
      # Set up spread constraints to try and spread the pods evenly across the nodes
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              {{ subresource_labels(name) | indent(14) }}
