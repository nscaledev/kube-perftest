apiVersion: batch/v1
kind: Job
metadata:
  name: {{ name }}-client
  labels:
    {{ subresource_labels(name, 'client') | indent(4) }}
spec:
  template:
    metadata:
      labels:
        {{ subresource_labels(name, 'client') | indent(8) }}
    spec:
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      restartPolicy: Never
      containers:
        - name: client
          image: {{ spec.image }}
          imagePullPolicy: {{ spec.imagePullPolicy }}
          securityContext:
            capabilities:
              add: [ "IPC_LOCK" ]
          args:
            - ib_{{ spec.mode }}_bw
            - -a
            - -b
            - {{ server_ip }}
          # Ask for the specified RDMA device
          resources:
            limits:
              rdma/{{ spec.rdmaSharedDeviceName }}: 1
      # Because these jobs are very performance sensitive, prevent the pod fromÂ being
      # scheduled onto a node that already has a kube-perftest component running
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/managed-by
                    operator: In
                    values:
                      - perftest-operator
              topologyKey: "kubernetes.io/hostname"
      nodeSelector:
        # Only run on nodes where RDMA is available
        feature.node.kubernetes.io/custom-rdma.available: "true"
        {%- if spec.nodeSelector %}
        {%- for label, value in spec.nodeSelector %}
        {{ label }}: "{{ value }}"
        {%- endfor %}
        {%- endif %}
      {%- if spec.tolerations %}
      tolerations:
        {{ spec.tolerations | tojson }}
      {%- endif %}
