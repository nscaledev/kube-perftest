{% import '_macros.j2' as macros %}

---
{{ macros.discovery_service(benchmark) }}
---
{{ macros.discovery_configmap(benchmark, master = 1, worker = benchmark.spec.num_nodes) }}
---
{{ macros.ssh_configmap(benchmark) }}
---
{% call macros.job(benchmark) -%}
minAvailable: {{ benchmark.spec.num_nodes + 1 }}
plugins:
  env: []
  ssh: []
tasks:
  - name: master
    replicas: 1
    policies:
      # When the master terminates successfully, the job is done
      - event: TaskCompleted
        action: CompleteJob
    template:
      metadata:
        labels:
          {{ macros.labels(benchmark, "master") | indent(10) }}
        {%- if not benchmark.spec.host_network and benchmark.spec.network_name %}
        annotations:
          v1.multus-cni.io/default-network: {{ benchmark.spec.network_name }}
        {%- endif %}
      spec:
        priorityClassName: {{ benchmark.status.priority_class_name }}
        {%- if benchmark.spec.host_network %}
        hostNetwork: true
        dnsPolicy: ClusterFirstWithHostNet
        {%- endif %}
        restartPolicy: OnFailure
        initContainers:
          - {{ macros.discovery_hosts_init_container(benchmark) | indent(12) }}
          {{ macros.networking_mtu_init_container(benchmark) | indent(10) }}
          - name: openfoam-init
            image: {{ benchmark.spec.image }}
            imagePullPolicy: {{ benchmark.spec.image_pull_policy }}
            command:
              - openfoam-init
              - {{ benchmark.spec.problem_size }}
              - {{ benchmark.spec.iterative_method }}
              - "{{ benchmark.spec.num_procs }}"
              - /etc/kube-perftest/worker-hosts
            volumeMounts:
              {{ macros.discovery_volume_mounts(benchmark) | indent(14) }}
              - {{ macros.ssh_volume_mount(benchmark) | indent(16) }}
              - name: opt-benchmark
                mountPath: /opt/benchmark
        containers:
          - name: master
            image: {{ benchmark.spec.image }}
            imagePullPolicy: {{ benchmark.spec.image_pull_policy }}
            {%- if benchmark.spec.transport == "RDMA" %}
            env:
              - name: UCX_TLS
                value: self,rc_x
            {%- endif %}
            args:
              - mpi-master
              - --allow-run-as-root
              - --display-map
              - --hostfile
              - /etc/kube-perftest/worker-hosts
              - -np
              - "{{ benchmark.spec.num_procs }}"
              - --map-by
              - node
              - -x
              - PATH
              {%- if benchmark.spec.transport == "TCP" %}
              - --mca
              - btl
              - self,tcp
              - --mca
              - pml
              - ob1
              - --mca
              - osc
              - ^ucx
              {%- else %}
              - -x
              - UCX_TLS
              - --mca
              - btl
              - ^openib
              - --mca
              - pml
              - ucx
              - --mca
              - osc
              - ucx
              {%- endif %}
              - foamExec
              - icoFoam
              - -parallel
            {%- if benchmark.spec.resources %}
            resources:
              {{ benchmark.spec.resources | toyaml | indent(14) }}
            {%- endif %}
            volumeMounts:
              {{ macros.discovery_volume_mounts(benchmark) | indent(14) }}
              - {{ macros.ssh_volume_mount(benchmark) | indent(16) }}
              - name: opt-benchmark
                mountPath: /opt/benchmark
            workingDir: /opt/benchmark
        volumes:
          - {{ macros.discovery_volume(benchmark) | indent(12) }}
          - {{ macros.ssh_volume(benchmark) | indent(12) }}
          - name: opt-benchmark
            emptyDir: {}
        # The master should avoid pods from other benchmarks
        {{ macros.distribution_spread(benchmark, "master") | indent(8) }}

  - name: worker
    replicas: {{ benchmark.spec.num_nodes }}
    template:
      metadata:
        labels:
          {{ macros.labels(benchmark, "worker") | indent(10) }}
        {%- if not benchmark.spec.host_network and benchmark.spec.network_name %}
        annotations:
          v1.multus-cni.io/default-network: {{ benchmark.spec.network_name }}
        {%- endif %}
      spec:
        priorityClassName: {{ benchmark.status.priority_class_name }}
        {%- if benchmark.spec.host_network %}
        hostNetwork: true
        dnsPolicy: ClusterFirstWithHostNet
        {%- endif %}
        restartPolicy: OnFailure
        initContainers:
          - {{ macros.discovery_hosts_init_container(benchmark) | indent(12) }}
          {{ macros.networking_mtu_init_container(benchmark) | indent(10) }}
        containers:
          - name: worker
            image: {{ benchmark.spec.image }}
            imagePullPolicy: {{ benchmark.spec.image_pull_policy }}
            securityContext:
              capabilities:
                add: [ "IPC_LOCK" ]
            env:
              - name: SSH_PORT
                value: "{{ benchmark.spec.ssh_port }}"
            ports:
              - name: ssh
                containerPort: {{ benchmark.spec.ssh_port }}
                protocol: TCP
            args:
              - mpi-worker
            {%- if benchmark.spec.resources %}
            resources:
              {{ benchmark.spec.resources | toyaml | indent(14) }}
            {%- endif %}
            volumeMounts:
              {{ macros.discovery_volume_mounts(benchmark) | indent(14) }}
              - {{ macros.ssh_volume_mount(benchmark) | indent(16) }}
              - name: opt-benchmark
                mountPath: /opt/benchmark
        volumes:
          - {{ macros.discovery_volume(benchmark) | indent(12) }}
          - {{ macros.ssh_volume(benchmark) | indent(12) }}
          - name: opt-benchmark
            emptyDir: {}
        # The workers should avoid pods from other benchmarks and each other
        # Max skew of 1 with two workers ensures they will be on separate hosts
        {{ macros.distribution_spread(benchmark, "worker") | indent(8) }}
{%- endcall %}
